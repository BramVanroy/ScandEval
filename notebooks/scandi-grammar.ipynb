{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586eaf2-5763-4636-944d-94bdb490cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scandeval import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f028c-a9a0-4923-81c2-5479e28305ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flip_verbs(tokens: list, pos_tags: list) -> str:\n",
    "    \n",
    "    # Copy the token list\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    # Collect all indices that are verbs\n",
    "    indices = [idx for idx, pos_tag in enumerate(pos_tags) \n",
    "               if pos_tag in ['VERB', 'AUX']]\n",
    "    \n",
    "    # If there are fewer than two verbs then return None\n",
    "    if len(indices) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Get two random verb indices\n",
    "    rnd_fst_idx = random.choice(indices)\n",
    "    rnd_snd_idx = random.choice(\n",
    "        [idx for idx in indices if idx != rnd_fst_idx]\n",
    "    )\n",
    "        \n",
    "    # Flip the two indices\n",
    "    new_tokens[rnd_fst_idx] = tokens[rnd_snd_idx]\n",
    "    new_tokens[rnd_snd_idx] = tokens[rnd_fst_idx]\n",
    "    \n",
    "    return join_tokens(new_tokens)\n",
    "\n",
    "\n",
    "def flip_nouns(tokens: list, pos_tags: list) -> str:\n",
    "    \n",
    "    # Copy the token list\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    # Collect all indices that are nouns\n",
    "    indices = [idx for idx, pos_tag in enumerate(pos_tags) \n",
    "               if pos_tag  == 'NOUN']\n",
    "    \n",
    "    # If there are no relevant indices then return None\n",
    "    if len(indices) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Get two random noun indices\n",
    "    rnd_fst_idx = random.choice(indices)\n",
    "    rnd_snd_idx = random.choice(\n",
    "        [idx for idx in indices if idx != rnd_fst_idx]\n",
    "    )\n",
    "        \n",
    "    # Flip the two indices\n",
    "    new_tokens[rnd_fst_idx] = tokens[rnd_snd_idx]\n",
    "    new_tokens[rnd_snd_idx] = tokens[rnd_fst_idx]\n",
    "    \n",
    "    return join_tokens(new_tokens)\n",
    "\n",
    "\n",
    "def change_case(tokens: list, pos_tags: list) -> str:\n",
    "    \n",
    "    # Copy the token list\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    # Loop until candidate indices have been found\n",
    "    indices = list()\n",
    "    cases = ['lower', 'upper', 'title']\n",
    "    while len(indices) == 0 and len(cases) > 0:\n",
    "        \n",
    "        # Randomly choose what case to change\n",
    "        case_to_change = random.choice(cases)\n",
    "        \n",
    "        # Get the indices with the given case. We only change the case of nouns\n",
    "        # and proper nouns.\n",
    "        if case_to_change == 'lower':\n",
    "            indices = [idx for idx, token in enumerate(new_tokens)\n",
    "                       if pos_tags[idx] in ['NOUN', 'PROPN'] and token.lower() == token]\n",
    "        elif case_to_change == 'upper':\n",
    "            indices = [idx for idx, token in enumerate(new_tokens) \n",
    "                       if pos_tags[idx] in ['NOUN', 'PROPN'] and token.upper() == token]\n",
    "        elif case_to_change == 'title':\n",
    "            indices = [idx for idx, token in enumerate(new_tokens) \n",
    "                       if pos_tags[idx] in ['NOUN', 'PROPN'] and token.title() == token]\n",
    "        \n",
    "        # Ensure that there are letters in the tokens\n",
    "        indices = [idx for idx in indices if re.search('\\w', new_tokens[idx]) is not None]\n",
    "        \n",
    "        # If there were no tokens to change with the given case, remove it from the list of cases\n",
    "        if len(indices) == 0:\n",
    "            cases.remove(case_to_change)\n",
    "            \n",
    "    # If we ran out of possible cases then return None\n",
    "    if len(cases) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Randomly choose an index to change\n",
    "    rnd_idx = random.choice(indices)\n",
    "        \n",
    "    # Randomly choose a new casing\n",
    "    if case_to_change == 'lower':\n",
    "        if random.random() < 0.1:\n",
    "            new_case = 'upper'\n",
    "        else:\n",
    "            new_case = 'title'\n",
    "    elif case_to_change == 'upper':\n",
    "        new_case = random.choice(['lower', 'title'])\n",
    "    elif case_to_change == 'title':\n",
    "        if random.random() < 0.1:\n",
    "            new_case = 'upper'\n",
    "        else:\n",
    "            new_case = 'lower'\n",
    "    \n",
    "    # Change the case of the random token\n",
    "    token = new_tokens[rnd_idx]\n",
    "    if new_case == 'lower':\n",
    "        new_tokens[rnd_idx] = token.lower()\n",
    "    elif new_case == 'upper':\n",
    "        new_tokens[rnd_idx] = token.upper()\n",
    "    elif new_case == 'title':\n",
    "        new_tokens[rnd_idx] = token.title()\n",
    "        \n",
    "    return join_tokens(new_tokens)\n",
    "\n",
    "\n",
    "def change_verb_ending(tokens: list, pos_tags: list) -> str:\n",
    "    \n",
    "    # Copy the token list\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    # Create list of all vowels\n",
    "    vowels = list('aeiouyæøåäöáíóúý')\n",
    "    \n",
    "    # Collect all indices that are verbs ending in 'r'\n",
    "    indices_with_r = [idx for idx, pos_tag in enumerate(pos_tags) \n",
    "                      if pos_tag in ['VERB', 'AUX'] and \n",
    "                      new_tokens[idx][-1].lower() == 'r']\n",
    "    \n",
    "    # Collect all indices that are verbs ending in a vowel\n",
    "    indices_with_vowel = [idx for idx, pos_tag in enumerate(pos_tags) \n",
    "                          if pos_tag in ['VERB', 'AUX'] and \n",
    "                          new_tokens[idx][-1].lower() in vowels]\n",
    "    \n",
    "    # If there are no relevant indices then return None\n",
    "    if len(indices_with_r) + len(indices_with_vowel) == 0:\n",
    "        return None\n",
    "    \n",
    "    # If only the vowel list is non-empty, then choose it\n",
    "    elif len(indices_with_r) == 0:\n",
    "        chosen_list = indices_with_vowel\n",
    "\n",
    "    # If only the 'r' list is non-empty, then choose it\n",
    "    elif len(indices_with_vowel) == 0:\n",
    "        chosen_list = indices_with_r\n",
    "        \n",
    "    # Otherwise, choose a list at random\n",
    "    else:\n",
    "        chosen_list = random.choice([indices_with_vowel, indices_with_r])\n",
    "    \n",
    "    # Get a random index from the list\n",
    "    rnd_idx = random.choice(chosen_list)\n",
    "    \n",
    "    # If the given token ends with an 'r' then remove it\n",
    "    if new_tokens[rnd_idx][-1].lower() == 'r':\n",
    "        new_tokens[rnd_idx] = new_tokens[rnd_idx][:-1]\n",
    "        \n",
    "    # Otherwise the token ends with a vowel, and we add an 'r'\n",
    "    else:\n",
    "        new_tokens[rnd_idx] = new_tokens[rnd_idx] + 'r'\n",
    "        \n",
    "    return join_tokens(new_tokens)\n",
    "\n",
    "\n",
    "def change_double_consonant(tokens: list, pos_tags: list) -> str:\n",
    "    \n",
    "    # Copy the token list\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    # Create list of consonants\n",
    "    consonants = list('qwrtpsdfghjklzxcvbnmðþ')\n",
    "    \n",
    "    # Collect all indices that are words with a consecutive consonants\n",
    "    indices_with_double_consonants = [\n",
    "        idx for idx, token in enumerate(new_tokens)\n",
    "        if any(i > 0 and token[i] == token[i+1] and token[i].lower() in consonants\n",
    "               for i in range(len(token) - 1))\n",
    "    ]\n",
    "    \n",
    "    # Collect all indices that are words with single consonants\n",
    "    indices_with_single_consonants = [\n",
    "        idx for idx, token in enumerate(new_tokens)\n",
    "        if any((i == len(token) - 1 or token[i] != token[i+1]) and\n",
    "               token[i].lower() in consonants\n",
    "               for i in range(1, len(token)))\n",
    "    ]\n",
    "    \n",
    "    # If there are no indices with consonants then return None\n",
    "    if len(indices_with_single_consonants) + len(indices_with_double_consonants) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Otherwise, if there are no double consonants then our task will be\n",
    "    # to add a double consonant\n",
    "    elif len(indices_with_double_consonants) == 0:\n",
    "        indices = indices_with_single_consonants\n",
    "        \n",
    "    # Otherwise, if there are no single consonants then our task will be\n",
    "    # to remove a double consonant\n",
    "    elif len(indices_with_single_consonants) == 0:\n",
    "        indices = indices_with_double_consonants\n",
    "        \n",
    "    # If there are double consonants then choose at random whether to add\n",
    "    # or remove a double consonant\n",
    "    elif random.random() < 0.5:\n",
    "        indices = indices_with_single_consonants\n",
    "    else:\n",
    "        indices = indices_with_double_consonants\n",
    "    \n",
    "    # Get a random index from the list\n",
    "    rnd_idx = random.choice(indices)\n",
    "    new_token = new_tokens[rnd_idx]\n",
    "    \n",
    "    # Case 1: If we are adding a double consonant\n",
    "    if indices == indices_with_single_consonants:\n",
    "        \n",
    "        # Get a list of character indices with are consonants\n",
    "        char_indices = [\n",
    "            idx for idx in range(1, len(new_token))\n",
    "            if (idx == len(new_token) - 1 or new_token[idx] != new_token[idx+1]) and\n",
    "               new_token[idx].lower() in consonants\n",
    "        ]\n",
    "        \n",
    "        # Get a random character index from the list\n",
    "        rnd_char_idx = random.choice(char_indices)\n",
    "        \n",
    "        # Duplicate the consonant\n",
    "        new_token = (new_tokens[rnd_idx][:rnd_char_idx] + \n",
    "                     new_tokens[rnd_idx][rnd_char_idx] +\n",
    "                     new_tokens[rnd_idx][rnd_char_idx:])\n",
    "        new_tokens[rnd_idx] = new_token\n",
    "        \n",
    "    # Case 2: If we are deleting a double consonant\n",
    "    else:\n",
    "        \n",
    "        # Get list of character indices which begins a double consonant\n",
    "        char_indices = [idx for idx, char in enumerate(new_tokens[rnd_idx][:-1])\n",
    "                        if char.lower() in consonants and char == new_tokens[rnd_idx][idx+1]]\n",
    "\n",
    "        # Get a random character index from the list\n",
    "        rnd_char_idx = random.choice(char_indices)\n",
    "\n",
    "        # Remove the letter from the token\n",
    "        new_token = ''.join(\n",
    "            [char for idx, char in enumerate(new_tokens[rnd_idx]) \n",
    "             if idx != rnd_char_idx]\n",
    "        )\n",
    "        new_tokens[rnd_idx] = new_token\n",
    "        \n",
    "    return join_tokens(new_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fc649-3b10-4296-9b3e-0b049f1237bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def join_tokens(tokens: list) -> str:\n",
    "    \n",
    "    # Form document\n",
    "    doc = ' '.join(tokens)\n",
    "    \n",
    "    # Remove whitespace around punctuation\n",
    "    doc = (doc.replace(' .', '.')\n",
    "              .replace(' ,', ',')\n",
    "              .replace(' ;', ';')\n",
    "              .replace(' :', ':')\n",
    "              .replace('( ', '(')\n",
    "              .replace(' )', ')')\n",
    "              .replace('[ ', '[')\n",
    "              .replace(' ]', ']')\n",
    "              .replace('{ ', '{')\n",
    "              .replace(' }', '}')\n",
    "              .replace(' ?', '?')\n",
    "              .replace(' !', '!'))\n",
    "    \n",
    "    # Remove whitespace around quotes\n",
    "    if doc.count('\"') % 2 == 0:\n",
    "        doc = re.sub('\" ([^\"]*) \"', '\"\\\\1\"', doc)\n",
    "        \n",
    "    return doc\n",
    "\n",
    "\n",
    "def delete(tokens: list, pos_tags: list) -> str:\n",
    "    \n",
    "    # Copy the token list\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    # Get candidate indices to remove. We do not remove adjectives,\n",
    "    # adverbs, punctuation, determiners or numbers, as the resulting sentence \n",
    "    # will probably still be grammatically correct. Further, we do not \n",
    "    # remove nouns or proper nouns if they have another noun or proper\n",
    "    # noun as neighbour, as that usually does not make the sentence\n",
    "    # incorrect either.\n",
    "    indices = [\n",
    "        idx for idx, pos_tag in enumerate(pos_tags)\n",
    "        if pos_tag not in ['ADJ', 'ADV', 'PUNCT', 'SYM', 'DET', 'NUM'] and\n",
    "        (pos_tag not in ['NOUN', 'PROPN'] or \n",
    "         ((idx == 0 or pos_tags[idx - 1] not in ['NOUN', 'PROPN']) and \n",
    "          (idx == len(new_tokens) - 1 or pos_tags[idx + 1] not in ['NOUN', 'PROPN'])))\n",
    "    ]\n",
    "        \n",
    "    # If there are no candidates then return None\n",
    "    if len(indices) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get the random index\n",
    "    rnd_idx = random.choice(indices)\n",
    "        \n",
    "    # Delete the token at the index\n",
    "    new_tokens.pop(rnd_idx)\n",
    "\n",
    "    return join_tokens(new_tokens)\n",
    "\n",
    "\n",
    "def flip_neighbours(tokens: list, pos_tags: list) -> str:\n",
    "    \n",
    "    # Copy the token list\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    # Collect all indices that are proper words, and which\n",
    "    # has a neighbour which is also a proper word as well as having\n",
    "    # a different POS tag\n",
    "    indices = [idx for idx, pos_tag in enumerate(pos_tags) \n",
    "               if pos_tag not in ['PUNCT', 'SYM']]\n",
    "    indices = [idx for idx in indices \n",
    "               if (idx + 1 in indices and pos_tags[idx] != pos_tags[idx + 1]) or \n",
    "                  (idx - 1 in indices and pos_tags[idx] != pos_tags[idx - 1])]\n",
    "    \n",
    "    # If there are fewer than two relevant tokens then return None\n",
    "    if len(indices) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Get the first random index\n",
    "    rnd_fst_idx = random.choice(indices)\n",
    "    \n",
    "    # Get the second (neighbouring) index\n",
    "    if rnd_fst_idx == 0:\n",
    "        rnd_snd_idx = rnd_fst_idx + 1\n",
    "    elif rnd_fst_idx == len(tokens) - 1:\n",
    "        rnd_snd_idx = rnd_fst_idx - 1\n",
    "    elif (pos_tags[rnd_fst_idx + 1] in ['PUNCT', 'SYM'] or \n",
    "          pos_tags[rnd_fst_idx] == pos_tags[rnd_fst_idx + 1] or\n",
    "          {pos_tags[rnd_fst_idx], pos_tags[rnd_fst_idx + 1]} == {'PRON', 'AUX'}):\n",
    "        rnd_snd_idx = rnd_fst_idx - 1\n",
    "    elif (pos_tags[rnd_fst_idx - 1] in ['PUNCT', 'SYM'] or \n",
    "          pos_tags[rnd_fst_idx] == pos_tags[rnd_fst_idx - 1] or\n",
    "          {pos_tags[rnd_fst_idx], pos_tags[rnd_fst_idx + 1]} == {'PRON', 'AUX'}):\n",
    "        rnd_snd_idx = rnd_fst_idx + 1\n",
    "    elif random.random() > 0.5:\n",
    "        rnd_snd_idx = rnd_fst_idx - 1\n",
    "    else:\n",
    "        rnd_snd_idx = rnd_fst_idx + 1\n",
    "        \n",
    "    # Flip the two indices\n",
    "    new_tokens[rnd_fst_idx] = tokens[rnd_snd_idx]\n",
    "    new_tokens[rnd_snd_idx] = tokens[rnd_fst_idx]\n",
    "    \n",
    "    # If we flipped the first character, then ensure that the new first character\n",
    "    # is title-cased and the second character is of lower case. We only do this if\n",
    "    # they are not upper cased, however.\n",
    "    if rnd_fst_idx == 0 or rnd_snd_idx == 0:\n",
    "        if new_tokens[0] != new_tokens[0].upper():\n",
    "            new_tokens[0] = new_tokens[0].title()\n",
    "        if new_tokens[1] != new_tokens[1].upper():\n",
    "            new_tokens[1] = new_tokens[1].lower()\n",
    "    \n",
    "    return join_tokens(new_tokens)  \n",
    "\n",
    "\n",
    "def corrupt(tokens: list, pos_tags: list, num_corruptions: int = 3) -> list:\n",
    "    corruptions = list()\n",
    "    while len(corruptions) < num_corruptions:\n",
    "        corruption_fn = random.choice([\n",
    "            flip_neighbours, \n",
    "            delete,\n",
    "        ])\n",
    "        corruption = corruption_fn(tokens, pos_tags)\n",
    "        if corruption not in corruptions and corruption is not None:\n",
    "            corruptions.append((corruption, corruption_fn.__name__))\n",
    "    return corruptions\n",
    "    \n",
    "    \n",
    "def prepare_df(df, split: str):\n",
    "    corrupted_list = [corrupt(tokens=tokens, pos_tags=pos_tags, num_corruptions=1)\n",
    "                      for tokens, pos_tags in zip(df.tokens, df.pos_tags)]\n",
    "    df['corrupted'] = [[tup[0] for tup in lst] for lst in corrupted_list]\n",
    "    df['corruption_type'] = [[tup[1] for tup in lst] for lst in corrupted_list]\n",
    "    df = pd.concat([\n",
    "        pd.DataFrame(dict(text=df.tokens.map(join_tokens).tolist(), \n",
    "                          corruption_type=[None for _ in range(len(df))], \n",
    "                          label=['correct' for _ in range(len(df))])),\n",
    "        pd.DataFrame(dict(text=df.corrupted.explode().tolist(), \n",
    "                          corruption_type=df.corruption_type.explode().tolist(), \n",
    "                          label=['incorrect' for _ in range(len(df))]))\n",
    "    ]).sample(frac=1.0).reset_index(drop=True)\n",
    "    return Dataset.from_pandas(df, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35cac3-07d6-44bf-ba22-514ac725d53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X_train, X_test, y_train, y_test = load_dataset('ddt-pos')\n",
    "\n",
    "# Concatenate the POS tags to the tokens\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Remove samples with five or fewer tokens\n",
    "train_df = train_df[train_df.tokens.map(lambda lst: len(lst) > 5)]\n",
    "test_df = test_df[test_df.tokens.map(lambda lst: len(lst) > 5)]\n",
    "\n",
    "# Remove samples with five or fewer distinct POS tags\n",
    "train_df = train_df[train_df.pos_tags.map(lambda lst: len(set(lst)) > 5)]\n",
    "test_df = test_df[test_df.pos_tags.map(lambda lst: len(set(lst)) > 5)]\n",
    "\n",
    "# Remove samples with an odd number of quotes\n",
    "train_df = train_df[train_df.doc.map(lambda doc: doc.count('\"') % 2 == 0)]\n",
    "test_df = test_df[test_df.doc.map(lambda doc: doc.count('\"') % 2 == 0)]\n",
    "\n",
    "# Remove samples which starts with punctuation\n",
    "train_df = train_df[train_df.pos_tags.map(lambda lst: lst[0] not in ['PUNCT', 'SYM'])]\n",
    "test_df = test_df[test_df.pos_tags.map(lambda lst: lst[0] not in ['PUNCT', 'SYM'])]\n",
    "\n",
    "# Remove samples containing the more than one '=' character, as this\n",
    "# is used to indicate a tag\n",
    "train_df = train_df[train_df.doc.map(lambda doc: doc.count('=') <= 1)]\n",
    "test_df = test_df[test_df.doc.map(lambda doc: doc.count('=') <= 1)]\n",
    "\n",
    "# Shuffle the samples and reset the index\n",
    "train_df = train_df.sample(frac=1.0).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4165c-aadd-498e-b46a-bff05520a4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IDX = random.choice(train_df.index)\n",
    "print('Sample:', IDX)\n",
    "\n",
    "tokens = train_df.iloc[IDX].tokens\n",
    "doc = join_tokens(tokens)\n",
    "pos_tags = train_df.iloc[IDX].pos_tags\n",
    "corrupted = corrupt(tokens, pos_tags, num_corruptions=1)[0]\n",
    "\n",
    "print(f'Original: \"{doc}\"')\n",
    "print(f'Corrupted: \"{corrupted[0]}\"')\n",
    "print(f'Method: {corrupted[1]}')\n",
    "print('POS tags:')\n",
    "print(list(zip(tokens, pos_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f05ce-2d51-481a-bd7b-51994d342e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\n",
    "    ('da', 'ddt-pos'),\n",
    "    ('sv', 'sdt-pos'),\n",
    "    ('nb', 'ndt-nb-pos'),\n",
    "    ('nn', 'ndt-nn-pos'),\n",
    "    ('is', 'idt-pos'),\n",
    "    ('fo', 'fdt-pos')\n",
    "]\n",
    " \n",
    "# Loop over all the datasets\n",
    "for language, dataset_id in tqdm(all_datasets):\n",
    "    \n",
    "    # Load the dataset\n",
    "    X_train, X_test, y_train, y_test = load_dataset(dataset_id)\n",
    "    \n",
    "    # Concatenate the POS tags to the tokens\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    test_df = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    # Remove samples with five or fewer tokens\n",
    "    train_df = train_df[train_df.tokens.map(lambda lst: len(lst) > 5)]\n",
    "    test_df = test_df[test_df.tokens.map(lambda lst: len(lst) > 5)]\n",
    "    \n",
    "    # Remove samples with five or fewer distinct POS tags\n",
    "    train_df = train_df[train_df.pos_tags.map(lambda lst: len(set(lst)) > 5)]\n",
    "    test_df = test_df[test_df.pos_tags.map(lambda lst: len(set(lst)) > 5)]\n",
    "    \n",
    "    # Remove samples with an odd number of quotes\n",
    "    train_df = train_df[train_df.doc.map(lambda doc: doc.count('\"') % 2 == 0)]\n",
    "    test_df = test_df[test_df.doc.map(lambda doc: doc.count('\"') % 2 == 0)]\n",
    "    \n",
    "    # Remove samples which starts with punctuation\n",
    "    train_df = train_df[train_df.pos_tags.map(lambda lst: lst[0] not in ['PUNCT', 'SYM'])]\n",
    "    test_df = test_df[test_df.pos_tags.map(lambda lst: lst[0] not in ['PUNCT', 'SYM'])]\n",
    "\n",
    "    # Remove samples containing the more than one '=' character, as this\n",
    "    # is used to indicate a tag\n",
    "    train_df = train_df[train_df.doc.map(lambda doc: doc.count('=') <= 1)]\n",
    "    test_df = test_df[test_df.doc.map(lambda doc: doc.count('=') <= 1)]\n",
    "    \n",
    "    # Remove samples containing 'SLUTORD', as this is used to indicate a tag\n",
    "    train_df = train_df[~train_df.doc.str.contains('SLUTORD')]\n",
    "    test_df = test_df[~test_df.doc.str.contains('SLUTORD')]\n",
    "    \n",
    "    # Shuffle the samples and reset the index\n",
    "    train_df = train_df.sample(frac=1.0).reset_index(drop=True)\n",
    "    test_df = test_df.sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "    # Create a validation set, and a small copy of the training set\n",
    "    val_df = train_df.iloc[-128:]\n",
    "    train_df = train_df.iloc[:-128]\n",
    "    small_train_df = train_df.copy().iloc[:512]\n",
    "    test_df = test_df.iloc[:512]\n",
    "\n",
    "    # Prepare the datasets by adding corruptions\n",
    "    small_train = prepare_df(small_train_df, split='small_train')\n",
    "    train = prepare_df(train_df, split='train')\n",
    "    val = prepare_df(val_df, split='val')\n",
    "    test = prepare_df(test_df, split='test')\n",
    "    \n",
    "    # Collect datasets in a dataset dictionary\n",
    "    dataset = DatasetDict(\n",
    "        small_train=small_train,\n",
    "        train=train,\n",
    "        val=val,\n",
    "        test=test\n",
    "    )\n",
    "\n",
    "    # Push the dataset to the Hugging Face Hub\n",
    "    dataset.push_to_hub(f'ScandEval/scala-{language}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935e628-e5a4-4a55-aa13-5b7898aa5eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
